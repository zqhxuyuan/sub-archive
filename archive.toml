[cli]
block_num=0
# if we specify an old block_num, but the database already has some data, and if we don't want to sync from
# the old block_num, so we can ignore it. in this way, the beginning sync block is the max block in database.
reset_force=false
# sync strategy
# - None: disable sync flow
# - ReadonlyBackend: use rocksdb readonly backend as the ArchiveCore's backend implementation
# - LocalBackend: use local backend which same as the block import's backend
strategy="ReadonlyBackend"
# runtime
# - RuntimeApi: using bin/archive/runtime generated RuntimeApi
# - MockRuntimeApi: using client/core/runtime/mock.rs
# - other RuntimeApi from external import
runtime="MockRuntimeApi"

[rocksdb]
#path = "../polkadot/database/chains/polkadot/db"
#path = "/tmp/dot100/chains/polkadot/db"
path = "/tmp/dot01/chains/polkadot/db"
cache_size = 128
secondary_db_path = "/tmp/dot01_r"

##################################
# Archive postgres configuration #
##################################
[postgres]
#uri = "postgres://koushiro:123@localhost:5432/kusama-archive"
uri = "postgres://postgres:postgres@192.168.2.142:5432/archive"
min_connections = 2
max_connections = 8
connect_timeout = 10
disable_statement_logging = true

##################################
# Archive kafka configuration #
##################################
[kafka]
queue_timeout = 0
topic = { metadata = "dot-metadata", block = "dot-block", finalized_block = "dot-finalized-block" }

## Broker:
##   `message.max.bytes`: default 1048588 (1 MB)
##   `replica.fetch.max.bytes`: default 1048576 (1 MiB)
## Producer:
##   `message.max.bytes`: default 1048576 (1 MiB)
## Consumer:
##   `max.partition.fetch.bytes`: default 1048576 (1 MiB)
##   `fetch.max.bytes`: default 57671680 (55 MiB)
[kafka.rdkafka]
"metadata.broker.list" = "192.168.2.142:9092" # or "bootstrap.servers" = "localhost:9092"
"compression.codec" = "gzip" # "none" | "gzip" | "snappy" | "lz4" | "zstd"
"message.max.bytes" = "10485760" # 10 MB